{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da238762-107f-4895-a31d-922531a8ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d660719f-2bec-40e9-96b9-fe561e46e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f329969-acf7-47b2-82da-3a26944802e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "\n",
    "xDIR_CHECKPOINT = \"gemma_3n_kinyarwanda_checkpoints\"\n",
    "#xDIR_CHECKPOINT = \"gemma_3n_kinyarwanda_checkpoints_16Bit\"\n",
    "\n",
    "xFile_examples = '/workspace/work/test_epoch_3_samples10000.txt'\n",
    "\n",
    "xEPOCHS = 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e17dce-b4f9-441e-9229-9f5af096dbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8225f1d0-6405-468e-bae4-0636459bd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.2.1: Fast Gemma3N patching. Transformers: 4.57.1. vLLM: 0.11.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.543 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4e48cd14754d6da695a302182b5dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load Model & Processor\n",
    "# -------------------------\n",
    "\n",
    "xmodel = \"unsloth/gemma-3n-E4B-it\"\n",
    "\n",
    "# 1. Load Model\n",
    "model, processor = FastModel.from_pretrained(\n",
    "    model_name = xmodel ,\n",
    "    load_in_4bit = True, # False #True, we try not 4bit to see how it goes \n",
    "    max_seq_length = 2048, # Audio tokens take up space, don't go too small\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300108ad-60c8-4e29-8b16-7d1c599c35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "# 2. Add LoRA Adapters\n",
    "# --------------------\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, \n",
    "    finetune_language_layers   = True, \n",
    "    finetune_attention_modules = True, \n",
    "    finetune_mlp_modules       = True, \n",
    "    r = 8, lora_alpha = 16, lora_dropout = 0,\n",
    "    bias = \"none\", random_state = 3407,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        # Audio specific layers\n",
    "        \"post\", \"linear_start\", \"linear_end\", \"embedding_projection\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950895d1-a3b1-4cdc-922c-7826007fe299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab1455847d34a81a25bccf9b2353c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 10000 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Load Your Pre-processed Data\n",
    "# -------------------------------\n",
    "#xFld = '/workspace/work/AI_Training_dset/audio_gemma/train/' #   2500 samples \n",
    "#xFld = '/workspace/work/AI_Training_dset/audio_gemma/train_samples_5000/'  # 5000 samples \n",
    "#xFld = '/workspace/work/AI_Training_dset/audio_gemma/train_samples_7500/'  # 7.500 samples \n",
    "xFld = '/workspace/work/AI_Training_dset/audio_gemma/train_samples_10000/'  # 10000\n",
    "\n",
    "\n",
    "dataset = load_from_disk(xFld)\n",
    "\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "print(f\"Loaded dataset with {len(dataset)} rows.\")\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae8e397-f8c5-46de-ac7a-6bbcd2636580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the Custom Data Collator (CRITICAL STEP)\n",
    "# --------------------------------------------------\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    audios = []\n",
    "\n",
    "    for example in examples:\n",
    "        # Get the messages list\n",
    "        msgs = example[\"messages\"]\n",
    "        \n",
    "        # 4a. Fix Roles (Safety Step)\n",
    "        # Standardize 'model' to 'assistant' for the chat template if needed\n",
    "        # (Gemma often uses 'model' natively, but HF templates usually expect 'assistant')\n",
    "        cleaned_msgs = []\n",
    "        for m in msgs:\n",
    "            role = \"assistant\" if m[\"role\"] == \"model\" else m[\"role\"]\n",
    "            content = m[\"content\"]\n",
    "            cleaned_msgs.append({\"role\": role, \"content\": content})\n",
    "\n",
    "        # 4b. Extract Text using Chat Template\n",
    "        text = processor.apply_chat_template(\n",
    "            cleaned_msgs, tokenize = False, add_generation_prompt = False\n",
    "        ).strip()\n",
    "        texts.append(text)\n",
    "\n",
    "        # 4c. Extract Audio (Specific to your structure)\n",
    "        # Your structure: messages[0] -> content[0] -> 'audio' -> [0.0, ...]\n",
    "        # We assume the audio is always in the first message's first content block\n",
    "        try:\n",
    "            audio_data = msgs[0][\"content\"][0][\"audio\"]\n",
    "            if audio_data is None:\n",
    "                 # Fallback search if audio isn't in the exact expected spot\n",
    "                 for m in msgs:\n",
    "                    for c in m[\"content\"]:\n",
    "                        if c.get(\"type\") == \"audio\" and c.get(\"audio\") is not None:\n",
    "                            audio_data = c[\"audio\"]\n",
    "                            break\n",
    "            audios.append(audio_data)\n",
    "        except (KeyError, IndexError):\n",
    "            print(\"Warning: Could not find audio in example.\")\n",
    "            audios.append(None) # This might crash the processor, ensure data is clean\n",
    "\n",
    "    # 5. Batch Process (Tokenize Text + Encode Audio)\n",
    "    # -----------------------------------------------\n",
    "    batch = processor(\n",
    "        text = texts, \n",
    "        audio = audios, \n",
    "        return_tensors = \"pt\", \n",
    "        padding = True\n",
    "    )\n",
    "\n",
    "    # 6. Create Labels & Masking\n",
    "    # --------------------------\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask padding\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Mask special multimodal tokens (so we don't try to predict the audio placeholder)\n",
    "    if hasattr(processor.tokenizer, 'image_token_id'):\n",
    "        labels[labels == processor.tokenizer.image_token_id] = -100\n",
    "    if hasattr(processor.tokenizer, 'audio_token_id'):\n",
    "        labels[labels == processor.tokenizer.audio_token_id] = -100\n",
    "    if hasattr(processor.tokenizer, 'boi_token_id'):\n",
    "        labels[labels == processor.tokenizer.boi_token_id] = -100\n",
    "    if hasattr(processor.tokenizer, 'eoi_token_id'):\n",
    "        labels[labels == processor.tokenizer.eoi_token_id] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e0e4ef-fe78-4807-a783-53296460dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Configure Trainer\n",
    "# --------------------\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = processor.tokenizer,\n",
    "    data_collator = collate_fn,  # Pass the function we defined above\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 20,\n",
    "        #max_steps = 300,\n",
    "        num_train_epochs = xEPOCHS, # 1 we test with two epochs \n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 50,\n",
    "        output_dir = xDIR_CHECKPOINT,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "        \n",
    "        # Audio Tuning Requirements\n",
    "        remove_unused_columns = False, \n",
    "        dataset_text_field = \"\",  # Leave empty because we use a collator\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True}, # We processed data manually\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68523bf-700a-40b4-8eb9-66973835f6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.543 GB.\n",
      "11.973 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5491fce1-ebae-4584-be25-e6f06c14ca86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,000 | Num Epochs = 3 | Total steps = 3,750\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 21,188,608 of 7,871,166,800 (0.27% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 49:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.221700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. Run Training\n",
    "# ---------------\n",
    "print(\"Starting training...\")\n",
    "#trainer_stats = trainer.train()\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4abd897b-93f7-4429-a9e9-bd2393016867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2987.2172 seconds used for training.\n",
      "49.79 minutes used for training.\n",
      "Peak reserved memory = 12.547 GB.\n",
      "Peak reserved memory for training = 0.574 GB.\n",
      "Peak reserved memory % of max memory = 53.294 %.\n",
      "Peak reserved memory for training % of max memory = 2.438 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ace01-13b9-491e-9db0-bb21c8eac50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee94e8de-41d6-479a-8be5-c21b69f2daa9",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0ba705-fbec-4f50-b9c1-50c9a9da335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import librosa  # We need this for resampling if your audio isn't 16k\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde6a04e-b3c8-4370-acc0-32d0863abdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio_id', 'audio', 'transcription', 'sampling_rate'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA \n",
    "\n",
    "### HERE WE TAKE 10 random samples \n",
    "\n",
    "xFld_test = '/workspace/work/AI_Training_dset/audio_badrex_kinyarwanda-speech-1000h/validation'\n",
    "\n",
    "#cache \n",
    "#cache_file = \"/tmp/shuffled_test.arrow\"\n",
    "ds = load_from_disk(xFld_test)\n",
    "#xdset = ds.shuffle(seed=50, indices_cache_file_name=cache_file).select(range(20))\n",
    "xdset = ds.shuffle(seed=50).select(range(20))\n",
    "\n",
    "xdset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d49a5a70-bd29-475e-a248-f11c8f807265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 2. DEFINE PROMPTS & RESAMPLER\n",
    "# ------------------------------------------------------------------------\n",
    "system_prompt = \"You are an assistant that transcribes speech accurately.\"\n",
    "user_instruction = \"Please transcribe this Kinyarwanda audio.\"\n",
    "\n",
    "def ensure_16k(audio_data, source_sr):\n",
    "    \"\"\"\n",
    "    Gemma-3N expects 16kHz audio. \n",
    "    If your dataset says sampling_rate is 44100 or 48000, we must resample.\n",
    "    \"\"\"\n",
    "    audio_np = np.array(audio_data, dtype=np.float32)\n",
    "    \n",
    "    if source_sr != 16000:\n",
    "        # Using librosa to resample quickly\n",
    "        audio_np = librosa.resample(audio_np, orig_sr=source_sr, target_sr=16000)\n",
    "        \n",
    "    return audio_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3660f341-127b-48a8-8977-44a2a4901fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CRITICAL: Set Padding Side to LEFT for Generation\n",
    "# Decoder-only models (like Gemma) generate from left to right. \n",
    "# If you pad on the right, the model sees [audio, pad, pad] and gets confused.\n",
    "# It must be [pad, pad, audio] so generation starts immediately after the audio tokens.\n",
    "processor.tokenizer.padding_side = \"left\" \n",
    "\n",
    "def run_stable_batch_inference(dataset, batch_size=4):\n",
    "    # 2. Sort dataset by audio length to minimize padding\n",
    "    # This reduces the chance of \"out of bounds\" errors caused by massive padding gaps\n",
    "    # We add a temporary column for length, sort, and then remove it\n",
    "    dataset = dataset.map(lambda x: {\"len\": len(x[\"audio\"])})\n",
    "    dataset = dataset.sort(\"len\", reverse=True) # Process longest first (often helps OOM)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    # Iterate with the sorted dataset\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing Batches\"):\n",
    "        batch = dataset[i : i + batch_size]\n",
    "        \n",
    "        batch_audio_lists = batch['audio']\n",
    "        batch_references = batch['transcription']\n",
    "        batch_srs = batch['sampling_rate']\n",
    "        \n",
    "        prompts_text = []\n",
    "        processed_audios = []\n",
    "        \n",
    "        for j, raw_audio_list in enumerate(batch_audio_lists):\n",
    "            current_sr = batch_srs[j]\n",
    "            audio_array = ensure_16k(raw_audio_list, current_sr)\n",
    "            processed_audios.append(audio_array)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"audio\", \"audio\": audio_array}, \n",
    "                    {\"type\": \"text\", \"text\": user_instruction}\n",
    "                ]}\n",
    "            ]\n",
    "            prompts_text.append(\n",
    "                processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            )\n",
    "\n",
    "        # 3. Generate Inputs\n",
    "        inputs = processor(\n",
    "            text=prompts_text,\n",
    "            audio=processed_audios,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,          # This will now apply LEFT padding\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # 4. Run Generation\n",
    "        # We use a try-except block to gracefully handle any lingering shape errors\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=False,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            input_len = inputs.input_ids.shape[1]\n",
    "            new_tokens = generated_ids[:, input_len:]\n",
    "            decoded_output = processor.batch_decode(\n",
    "                new_tokens, \n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            \n",
    "            all_predictions.extend(decoded_output)\n",
    "            all_references.extend(batch_references)\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"setStorage\" in str(e):\n",
    "                print(f\"\\n[Warning] Batch {i//batch_size} failed with storage error. Retrying with batch_size=1...\")\n",
    "                # Fallback: If a specific batch fails, run its items one by one\n",
    "                for k in range(len(prompts_text)):\n",
    "                    single_pred = run_single_inference(\n",
    "                        prompts_text[k], processed_audios[k], model, processor\n",
    "                    )\n",
    "                    all_predictions.append(single_pred)\n",
    "                    all_references.append(batch_references[k])\n",
    "            else:\n",
    "                raise e # Re-raise if it's a different error\n",
    "\n",
    "    return all_predictions, all_references\n",
    "\n",
    "# Helper for the fallback mechanism\n",
    "def run_single_inference(text, audio, model, processor):\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        audio=[audio],\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False # No padding needed for single item\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "    \n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    return processor.batch_decode(generated_ids[:, input_len:], skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e6fb575-ddca-438a-b50d-da04b84bd106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: Sat Feb 14 10:53:09 2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:20<00:00,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end: Sat Feb 14 10:54:29 2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'my_dataset' is your variable name\n",
    "\n",
    "print('start:', time.asctime())\n",
    "\n",
    "predictions, references = run_stable_batch_inference(xdset, batch_size=2)\n",
    "\n",
    "print('end:', time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eeee4ca-f162-4b00-8ed6-c04b1ea778ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation ---\n",
      "WER: 0.1975\n",
      "CER: 0.0508\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 5. METRICS\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"\\n--- Evaluation ---\")\n",
    "wer_score = wer(references, predictions)\n",
    "cer_score = cer(references, predictions)\n",
    "\n",
    "print(f\"WER: {wer_score:.4f}\")\n",
    "print(f\"CER: {cer_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ca3edc-c79c-4e3b-a2a6-bd42e98a85c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ref:  Igikoresho cyifashishwa n'abaganga bavura amagufwa ndetse n'imvune mu gihe barimo kuvura abarwayi bagize imvune zo mu ivi. Aho iki gikoresho bacyambikwa kugira ngo kibafashe gukanda ndetse no gusubiranya amagufwa n'imikaya byagize ikibazo igihe umuntu agize imvune. Ibi bikagaragaza iterambere mu buvuzi aho ibikoresho bigezweho bikoreshwa mu kunoza iyi serivisi.\n",
      "Pred: Igikoresho kifashishwa n'abaganga bavura amagufwa ndetse n'imvune mu gihe barimo kuvura abarwayi bagize imvune zo mu ivi, aho iki gikoresho cyandikwa kugira ngo kibafashe gukanda ndetse no gusubiranya amagufwa n'ibikaya byagize ikibazo igihe umuntu ageze imvune, ibikagaragaza iterambere mu buvuzi aho ibikoresho byegizweho bikoreshwa mu kunoza iyi serivisi.\n",
      "\n",
      "Ref:  Umugore wambaye agapira k'umukara imbere ye hicaye undi wambaye ishati y'umutuku ndetse n'iy'umweru ari kumwambika agakoresho ku kuboko kifashishwa mu kumenya umuvuduko w'amaraso, imbere yabo hari ameza manini ateretseho indangururamajwi hirya yabo hari abandi bantu bari kubareba.\n",
      "Pred: Umugore wambaye agapira k'umukara imbere ye hicaye undi wambaye ishati y'umutuku ndetse n'iy'umweru ari kumwambika agakoresho kukuboko kifashijwe mu kumenya umuvuduko w'amaraso imbere yabo hari ameza manini ateretseho indangururamajwi hirya yabo hari abandi bantu bari kubareba.\n",
      "\n",
      "Ref:  Hari amwe mu mavuta akorwa mu bimera bitandukanye, twavuga nk'igikakarubamba, bifite ubushobozi bwo kuvura indwara zigiye zitandukanye, cyane cyane indwara zikunze kwibasira uruhu rwa muntu.\n",
      "Pred: Hari amwe mu mavuta akurwa mu bimera bitandukanye twavuga igikakarubamba bifite ubushobozi bwo kuvura indwara zigiye zitandukanye, cyane cyane ingorazi ikunze kwibasira uruhu rwa muntu.\n",
      "\n",
      "Ref:  Isoko, isoko ubona  ribicuruzwamo ibintu byinshi bitandukanye, aho hagaragaramo ibikapu ndetse n'imyenda y'abagabo ndetse n'abagore, kandi bikagaragara ko harimo umukobwa ndetse n'umugabo, bikaba bigaragara ko uwo mukobwa ari kugura ibyo yaje kureba mu isoko, ndetse umugabo akaba acuruje\n",
      "Pred: Isoko, isoko ubanakorerwa icuruzwemo ibintu byinshi bitandukanye, aho hagaragaramo ibikapu ndetse n'imyenda y'abagabo ndetse n'abagore, kandi bikagaragara ko harimo umukobwa ndetse n'umugabo, bikaba bigaragara ko uwo mukobwa ari kugura ibyo yaje kurema isoko ndetse umugabo akaba acuruje.\n",
      "\n",
      "Ref:  Uru ni urubuga rukoresha uburyo bw'ikoranabuhanga ruzwi ku izina rya I gura, aho ushobora kuba wahahira ibikoresho bigiye bitandukanye, yaba imyambaro, yaba ibikoresho by'ikoranabuhanga n'ibindi bigiye bitandukanye hifashishijwe uburyo bw'ikoranabuhanga. \n",
      "Pred: Uru ni urubuga rukoresha uburyo bw'ikoranabuhanga rwuzwi ku izina rya igura, aho ushobora kuba wahahira ibikoresho bigiye bitandukanye yaba imyambaro, yaba ibikoresho by'ikoranabuhanga n'ibindi bigiye bitandukanye hifashishijwe uburyo bw'ikoranabuhanga.\n",
      "\n",
      "Ref:  Inyubako nini irimo ibikoresho byinshi byo kwa muganga, harimo ibitanda, intebe, ameza n'amarido y'umweru, ibyo byose byifashishwa bari mu kwita ku babagana, babafasha ku burwayi bashobora kuba  bafite ubwo aribwo bwose.\n",
      "Pred: Inyubako nini irimo ibikoresho byinshi byo kwa muganga, harimo ibitanda, intebe, ameza, amarido y'umweru, ibyo byose byifashishwa barimo kwita ku babagana, babafasha ku burwayi bashobora kuba bafite ubwari bw'umwoso.\n",
      "\n",
      "Ref:  Inyubako irimo amatageri meza amanitseho amatapi yo kurambika hasi neza y'amabara atandukanye arimo ayafite amabara ajya gusa umweru, ajya gusa umutuku, ajya gusa ubururu ni amabara menshi agiye avangavanze kandi meza.\n",
      "Pred: Inyubako irimo amatajeri meza aditseho amapi yo korambika hasi meza y'amabara atandukanye harimo ayafite amabara ajya gusa umweru ajya gusa umutuku ajya gusa ubururu ni amabara aciye avangwa hanze h'ameza.\n",
      "\n",
      "Ref:  Kugira ibikorwa bihuza abantu bakuze benshi, bo mu kigero cy'imyaka imwe, bakungurana ibitekerezo, bakishimana, buri wese akagaragaza amarangamutima ye, ni byiza cyane kuko bituma ubuzima bwabo bukomeza kugenda neza, kandi bagafashanya kwiteza imbere.\n",
      "Pred: Kugira ibikorwa bihuza abantu bakuze benshi bo mu Kigali y'imyaka imwe bakungurana ibitekerezo bakwishimana, buri wese akagaragaza amarangamutima ye, ni byiza cyane kuko bituma ubuzima bwabo bukomeza kugenda neza kandi bagafashanya kwiteza imbere.\n",
      "\n",
      "Ref:  Umuganga wabihuguriwe mu bijyanye n'ubuzima bw'amenyo, hari umwana waje kwivuza kugira ngo barebe ko amenyo ye adafite ikibazo, mu gihe basanze hari arwaye bayakuremo kugira ngo hirindwe kwanduza andi cyangwa se abashe guhabwa imiti iyoroshya. \n",
      "Pred: Umuganga wabihuguriwe mu bijyanye n'ubuzima bw'amenyo, hari umwana waje kwivuza kugira ngo barebe ko amenyo ye adafite ikibazo mu gihe basanze hari arwaye bayakuremo kugira ngo herindwe kwanduza andi cyangwa se abashe guhabwa imiti yoroshye.\n",
      "\n",
      "Ref:  Iri ni iduka ricuruza imyambaro igiye itandukanye; harimo amakoti yo kwifubika, amakoti yo kujyana ku kazi, amashati, imipira ndetse n'ibindi by'ubwoko bwinshi; aho babikugezaho cyangwa ukaza guhaha muri iryo duka.\n",
      "Pred: Iri ni iduka ricuruza imyambaro igiye itandukanye, harimo amakoti yo kwifubika, amakoti yo kujyana ku kazi, amashati, imipira ndetse n'ibindi by'ubwoko bwinshi, aho babikugezaho cyangwa ukaza guhaha muri iryo duka.\n",
      "\n",
      "Ref:  Umubyeyi wikoreye indobo, ufite iyindi mu ntoki, ukenyeye igitenge, wambaye ishati y'amaboko magufiya, witeze igitambaro, urimo kwerekeza ahantu hari ibiti kandi harimo n'abantu, hari n'agatanda kahubatse.\n",
      "Pred: Umubyeyi wikorera indabo ufite indi mu ntoki, ukeneye gitenge, wambaye ishati y'amaboko magufiya, witeze igitambaro urimo kwerekeza ahantu hari ibiti kandi harimo n'abantu, hari n'agatanda kahubatse.\n",
      "\n",
      "Ref:  Iyi ni imodoka igurishwa; iyi modoka ikaba yitwa meresedesi benzi, iragurishwa kandi ikagurishwa kuri miliyoni makumyabiri n'eshatu zonyine, bivuze ngo ni amafaranga make nawe ukeneye imodoka nziza waza ukayigurira.\n",
      "Pred: Iyi ni imodoka igurishwa, iyi modoka ikaba yitwa Mercedes Benz, iragurishwa kandi igurishwa kuri miliyoni makumyabiri n'eshatu z'onyine, bivuze ngo ni amafaranga make ntabwo ukeneye iyi modoka nziza wazukaye yigurira.\n",
      "\n",
      "Ref:  Uru ni urubuga rukorera kuri murandasi ndetse n'ikoranabuhanga rugufasha kohereza, kwakira amafaranga aturutse mu mahanga bitagusabye kuva aho uherereye cyangwa gukora ingendo utateguye.\n",
      "Pred: Uru ni urubuga rukorera kuri murandasi ndetse n'ikoranabuhanga rugufasha kohereza kwakira amafaranga aturutse mu mahanga bitagusabye kuva aho uherereye cyangwa gukora ingendo utateguye.\n",
      "\n",
      "Ref:  Abanyabuzima bafite amakayi ndetse bambaye n'amataburiya abaranga, bafite amakaramu ndetse biragaragara ko bari mu nama kuko bicaye bakurikiye.\n",
      "Pred: Abanyabuzima bafite amakayi ndetse bambaye n'amataburiya baranga, bafite amakaramu ndetse biragaragara ko bari mu nama kuko bicaye bakurikiye.\n",
      "\n",
      "Ref:  Icyumba cy'inama kirimo intebe, ameza biteye birebana n'impapuro zirambitse ku meza, n'amazi yo kunywa arambitse ku meza n'insakazamajwi, n'insakazamashusho, n'amatara yaka menshi.\n",
      "Pred: Icyumba kinini, hakaba harimo intebe, ameza biteye birebana, n'impapuro zirambitse ku meza, n'amazi yo kunywa arambitse ku meza, n'insakazamajwi, n'insakazamashusho, n'amatara yaka menshi.\n",
      "\n",
      "Ref:  Umwana mutoya w'uruhinja amanitse ku munzani, barimo baramupima ibiro kuko nibyo byerekana ubuzima bw'umwana, umubyeyi arahagaze ari kumureba yambaye ishati ifite ibara risa umweru.\n",
      "Pred: Umwana mutoya uruhinja amanitse ku munzani barimo baramupima ibiro kuko n'ibyo byerekana ubuzima bw'umwana, umubyeyi arahagaze ari kureba yambaye ishati ifite ibara risa n'umweru.\n",
      "\n",
      "Ref:  Tugane ikigo cy'ubwishingizi cya ariyanse, ubu kirakorera mu Rwanda, ni ngombwa y'uko dushinganisha ibyacu kugira ngo nihabaho igihe byahuye n'impanuka, tuzabone ikitugoboka.\n",
      "Pred: Tugane ikigo cy'ubwishingizi cya Allianz, ubukira akorera mu Rwanda ni ngombwa y'uko dusenganesha ibyacu kugira ngo nihabaho igihe byahuye n'ipantoka tuzabone igitugoboka.\n",
      "\n",
      "Ref:  Umugore ufite ikivomesho agiteze ku mugezi amazi ari kumanuka ajyamo, iruhande rwe hari abana babiri bakiri batoya bareba uko bigenda.\n",
      "Pred: Umugore ufite icivumesho agiteze ku mugezi amazi ari kumanuka ajyamo, iruhande rwe hari abana babiri bakiri batoya, bareba uko bigenda.\n",
      "\n",
      "Ref:  Abantu babiri bahagaze ahantu hari za mudasobwa bafite telefoni mu ntoki barimo arazikanda noneho hari ameza agiye ateretseho izo mudasobwa.\n",
      "Pred: Abantu babiri bahagaze ahantu hari za mudasobwa, bafite terefone mu ntoki barimo barazikanda, noneho hari ameza ari ateetse izo mudasobwa.\n",
      "\n",
      "Ref:  Iyi ngiyi ni ameza bariraho. Aya meza ni meza pe ni umukara, akaba afite n'ibara ribengerana nka zahabu nziza cyane, ituruka mu gihugu cy'abaturanyi.\n",
      "Pred: Iyi ngiyi ni ameza baririraho, aya meza ni meza peye ni umukara, akaba afite n'ibara ribengerana nkaza habonza izac cyane ituruka mu gucy'abaturanyi.\n"
     ]
    }
   ],
   "source": [
    "# Optional: Print a few examples\n",
    "\n",
    "\n",
    "with open(xFile_examples , 'w') as xff:\n",
    "    xff.write(f\"WER: {wer_score:.4f}\")\n",
    "    xff.write('\\n')    \n",
    "    xff.write(f\"CER: {cer_score:.4f}\")\n",
    "    xff.write('\\n#############################')       \n",
    "    for i in range(min(20, len(predictions))):\n",
    "        xff.write(f\"\\nRef:  {references[i]}\")\n",
    "        xff.write('\\n\\n')\n",
    "        xff.write(f\"Pred: {predictions[i]}\")              \n",
    "        xff.write('\\n--------')        \n",
    "              \n",
    "\n",
    "for i in range(min(20, len(predictions))):\n",
    "    print(f\"\\nRef:  {references[i]}\")\n",
    "    print(f\"Pred: {predictions[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e47d1692-34b8-473e-895b-2cb3a3d761b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.0483989151917"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CombinedError= (0.4 * wer_score) + (0.6 * cer_score) \n",
    "\n",
    "Score = (1 - CombinedError) * 100\n",
    "\n",
    "Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b4729-5f0d-4b55-bfdc-baffbccbc7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0bc6647-dfad-4d66-b162-7df8d181fc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /workspace/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e340816fa21f4fa0a605263bef5b46cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06f6a07dd554b68af8027e63a78b5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:24<01:13, 24.64s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f383c5630c4241a1186476564dc270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:03<01:05, 32.77s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94ca25c2a88438ab988aa439604196e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:43<00:36, 36.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba7c710125046a28dcaefd7742d3b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:06<00:00, 31.61s/it]\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:29<00:00,  7.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/work/gemma_3n_kin_10000_epochs_3`\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"/workspace/work/gemma_3n_kin_10000_epochs_3\", processor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
